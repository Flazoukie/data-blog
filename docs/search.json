[
  {
    "objectID": "posts/kagglehub-download.html",
    "href": "posts/kagglehub-download.html",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "",
    "text": "kagglehub is a relatively new and more user-friendly package developed by Kaggle to simplify access to datasets, notebooks, and models. The kagglehub Python package offers a cleaner alternative.\nThis guide, quickly created with teh help of ChatGPT, shows you how to download Kaggle datasets in just a few lines of code and save you some messages with the LLM, so you won‚Äôt hit the limit too early while developing your project ;)"
  },
  {
    "objectID": "posts/kagglehub-download.html#what-is-kagglehub",
    "href": "posts/kagglehub-download.html#what-is-kagglehub",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "What is kagglehub?",
    "text": "What is kagglehub?\nkagglehub is a Python package developed by Kaggle that allows you to programmatically download datasets, notebooks, and models."
  },
  {
    "objectID": "posts/kagglehub-download.html#step-1-install-kagglehub",
    "href": "posts/kagglehub-download.html#step-1-install-kagglehub",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Step 1: Install kagglehub",
    "text": "Step 1: Install kagglehub\npip install kagglehub"
  },
  {
    "objectID": "posts/kagglehub-download.html#step-2-authenticate-with-kaggle",
    "href": "posts/kagglehub-download.html#step-2-authenticate-with-kaggle",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Step 2: Authenticate with Kaggle",
    "text": "Step 2: Authenticate with Kaggle\nTo use kagglehub, you need a Kaggle API token.\n\nOption A: Save the token file\nGo to your Kaggle account settings: https://www.kaggle.com/account\nScroll to ‚ÄúAPI‚Äù and click ‚ÄúCreate New API Token‚Äù\nSave the downloaded kaggle.json file to:\nLinux/macOS: ~/.kaggle/kaggle.json\nWindows: C:&lt;YourUsername&gt;.kaggle.json\nIf the .kaggle folder doesn‚Äôt exist, create it manually.\n\n\nOption B: Use environment variables\nimport os\n\nos.environ[\"KAGGLE_USERNAME\"] = \"your_username\"\nos.environ[\"KAGGLE_KEY\"] = \"your_key\"\nReplace ‚Äúyour_username‚Äù and ‚Äúyour_key‚Äù with your API credentials from kaggle.json."
  },
  {
    "objectID": "posts/kagglehub-download.html#step-3-download-a-dataset",
    "href": "posts/kagglehub-download.html#step-3-download-a-dataset",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Step 3: Download a Dataset",
    "text": "Step 3: Download a Dataset\nHere‚Äôs how to download your dataset:\nimport kagglehub\n\npath = kagglehub.dataset_download(\"yourdataset\")\n\nprint(\"Dataset downloaded to:\", path)\nThe dataset is automatically downloaded and unzipped into a cache directory (usually ~/.cache/kagglehub/)."
  },
  {
    "objectID": "posts/kagglehub-download.html#step-4-explore-the-files",
    "href": "posts/kagglehub-download.html#step-4-explore-the-files",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Step 4: Explore the Files",
    "text": "Step 4: Explore the Files\nimport os\n\nfor file in os.listdir(path):\n    print(file)"
  },
  {
    "objectID": "posts/kagglehub-download.html#bonus-reusability-and-automation",
    "href": "posts/kagglehub-download.html#bonus-reusability-and-automation",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Bonus: Reusability and Automation",
    "text": "Bonus: Reusability and Automation\n\nkagglehub also works for notebooks and models.\nFiles are cached and won‚Äôt be redownloaded unless updated.\nGreat for use in automated workflows or reproducible projects."
  },
  {
    "objectID": "posts/kagglehub-download.html#conclusion",
    "href": "posts/kagglehub-download.html#conclusion",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Conclusion",
    "text": "Conclusion\nkagglehub is a simple, effective alternative to the Kaggle CLI, especially for Python users who prefer to keep everything in code. With just a few lines, you can grab datasets and start analyzing!"
  },
  {
    "objectID": "posts/how-to-create-a-blog.html",
    "href": "posts/how-to-create-a-blog.html",
    "title": "How to Create a Blog Using Quarto",
    "section": "",
    "text": "Creating your own blog is a great way to share your data projects. With Quarto and GitHub Pages, you can easily publish a professional-looking blog for free, using simple Markdown and code.\nThis setup is ideal for:\n\nShowcasing data science or programming projects\nWriting technical tutorials (like this ;) )\nMaintaining a lightweight, fast, and version-controlled website\n\nIn this tutorial, I‚Äôll walk you through how to create your blog step by step using Quarto and GitHub Pages."
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#introduction",
    "href": "posts/how-to-create-a-blog.html#introduction",
    "title": "How to Create a Blog Using Quarto",
    "section": "",
    "text": "Creating your own blog is a great way to share your data projects. With Quarto and GitHub Pages, you can easily publish a professional-looking blog for free, using simple Markdown and code.\nThis setup is ideal for:\n\nShowcasing data science or programming projects\nWriting technical tutorials (like this ;) )\nMaintaining a lightweight, fast, and version-controlled website\n\nIn this tutorial, I‚Äôll walk you through how to create your blog step by step using Quarto and GitHub Pages."
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#what-youll-need",
    "href": "posts/how-to-create-a-blog.html#what-youll-need",
    "title": "How to Create a Blog Using Quarto",
    "section": "üß∞ What You‚Äôll Need",
    "text": "üß∞ What You‚Äôll Need\n\nTechnical Requirements\n\nInstall Quarto Quarto is the publishing engine. You can write in Markdown, embed code, and produce beautiful web pages, one great thing about Quarto is that it supports various languages like R, Python, Julia, and Observable JavaScript. Download and install it from the official site.\nCreate a GitHub account (if you don‚Äôt have one yet) You‚Äôll use GitHub to host your website for free via GitHub Pages.\nInstall Git (if not already installed) Quarto uses Git to publish your site to GitHub. Install Git and set it up locally.\n\n\nOptional but helpful: A code editor like VS Code or PyCharm makes writing .qmd files easier, but it‚Äôs not required. I am personally using PyCharm Community Edition (the free version of PyCharm) Download PyCharm."
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#step-by-step-create-your-blog",
    "href": "posts/how-to-create-a-blog.html#step-by-step-create-your-blog",
    "title": "How to Create a Blog Using Quarto",
    "section": "üöÄ Step-by-Step: Create Your Blog",
    "text": "üöÄ Step-by-Step: Create Your Blog\nWe‚Äôll now create and publish your blog in just a few steps.\n\n\n1. Create a New Quarto Website\nOpen your terminal and run the following command:\nquarto create-project my-blog --type website\ncd my-blog\nThis creates a new folder called ‚Äúmy-blog‚Äù with the basic structure of a Quarto website. You can rename my-blog to whatever you want. You should now have gotten the following files:\n\nindex.qmd (your homepage)\nabout.qmd (your About page)\n_quarto.yml (the configuration file that defines your site layout and style)\nA posts/ folder to hold your blog posts\n\n\nIf you didn‚Äôt get a posts/ folder, create it manually. You will need it to hold all your blog posts!\n\n\n\n2. Preview Your Website Locally\nTo check if everything worked correctly, run the following command to see your site in your browser:\nquarto preview\nThis launches a localhost preview. Any time you change a .qmd file and save it, the preview updates automatically. Remember, this is not your site yet, but just a local preview. Next I will guide you through creating a proper site using GitHub Pages.\n\n\n3. Configure the Site for GitHub Pages\nTo publish your site using GitHub Pages, you need to configure Quarto to render your output into a folder named docs/.\nIn the root of your project, open the _quarto.yml file and modify or add the following line:\nproject:\n  type: website\n  output-dir: docs\nThen render your site:\nquarto render\nQuarto will generate the site in a docs/ folder, which GitHub Pages can use for publishing.\n\n\n4. Push Your Site to GitHub\nCreate a new repository on GitHub (e.g., ‚Äúmy-blog‚Äù)\nInitialize Git and link your local project to your GitHub repo:\ngit init\ngit remote add origin https://github.com/yourusername/my-blog.git\ngit add .\ngit commit -m \"Initial commit\"\ngit push -u origin main\nDon‚Äôt forget to replace ‚Äúyourusername‚Äù with your actual username and ‚Äúmy-blog‚Äù with your actual blog name ;). üîê If you‚Äôre using HTTPS, GitHub will ask you for a token instead of a password. You can generate one here.\n\n\n5. Enable GitHub Pages\n\nGo to your GitHub repository\nClick on Settings ‚Üí Pages\nUnder Source, select:\n\n\nBranch: main\nFolder: /docs\n\nClick Save. After a few seconds, your site will be live at: https://yourusername.github.io/data-blog/\n\nNote: This is the path I chose to publish my blog, however there are alternative ways to do it, which may better satisfy your requirements, have a look here: GitHub Pages for Quarto."
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#create-your-first-blog-post",
    "href": "posts/how-to-create-a-blog.html#create-your-first-blog-post",
    "title": "How to Create a Blog Using Quarto",
    "section": "‚úçüèø Create Your First Blog Post",
    "text": "‚úçüèø Create Your First Blog Post\nIf you do not have a folder for your posts already, create a new one. I keep my posts inside the folder ‚Äúposts‚Äù, within my own project folder DataBlog. Now you can create a new file, which will become your first post. For example:\nposts/how-to-create-a-data-blog.qmd\nYou can start with this YAML header and modify it accordingly with your needs:\n---\ntitle: \"My first post title\"\ndate: 2025-05-11\ncategories: [tutorial, quarto, github, blogging]\nformat:\n  html:\n    toc: true\n    code-tools: true\n    theme: cosmo\n---\nThen write your content below using Markdown and code blocks as needed.\nWhen ready, render the site again:\nquarto render\nNow push the changes to GitHub:\ngit add .\ngit commit -m \"Add first blog post\"\ngit push\nVisit your blog again ‚Äî Congratulations, your new post is live! ‚úåüèø"
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#remove-the-demo-post",
    "href": "posts/how-to-create-a-blog.html#remove-the-demo-post",
    "title": "How to Create a Blog Using Quarto",
    "section": "üóëÔ∏è Remove the Demo Post",
    "text": "üóëÔ∏è Remove the Demo Post\nWhen you create a new Quarto website project, it often comes with a demo blog post inside the posts/ folder. This is just a placeholder to show you how blog posts work. It usually has a name like:\nposts/welcome/index.qmd\n‚ÄúYou‚Äôll probably want to remove the demo post to keep your site clean. I removed them manually from Pycharm, but you can removed them from your terminal using a command like:\nrm posts/welcome/index.qmd\nDon‚Äôt forget not to render and commit!\nquarto render\ngit add .\ngit commit -m \"Remove demo post\"\ngit push"
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#customize-the-site",
    "href": "posts/how-to-create-a-blog.html#customize-the-site",
    "title": "How to Create a Blog Using Quarto",
    "section": "üé® Customize the Site",
    "text": "üé® Customize the Site\nNow you may want to customize the site. To do this, you need to change the _quarto.yml file. Open it and change title, navigation bar, theme, and styling according to your wishes.\nFor example:\nproject:\n  type: website\n  output-dir: docs\n\nwebsite:\n  title: \"My Blog\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\nüß© Optional: Add an Image and About Info\nWrite a small biography in about.qmd. and add a personal image. Here you can also add your contacts.\nThis is how my about.qmd looks like:\n---\ntitle: \"About\"\nimage: flavia_cartoon.png\nabout:\n  template: jolla\n  links:\n    - icon: linkedin\n      text: LinkedIn\n      href: https://www.linkedin.com/in/flavia-felletti-phd-8066141b8/\n    - icon: github\n      text: Github\n      href: https://github.com/Flazoukie\n\n---\n\n## About This Blog\n\nWelcome to my blog! Here is where I share my journey in data science through hands-on projects, tutorials, and reflections.\nI hope you'll find something useful, inspiring, or entertaining for you here.\nFeel free to take inspiration!\nEnsure the image is copied into the docs/ folder so it shows up on GitHub Pages. Remember now to render to turn your Markdown files into a working website:\nquarto render\nThis generates a docs/ folder with all the HTML files that make up your website.\nDon‚Äôt forget to commit and push to GitHub to apply teh changes to your website!\ngit add .\ngit commit -m \"Customize the website\"\ngit push"
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#wrap-up",
    "href": "posts/how-to-create-a-blog.html#wrap-up",
    "title": "How to Create a Blog Using Quarto",
    "section": "‚úÖ Wrap-Up",
    "text": "‚úÖ Wrap-Up\nCongratulations! üéâ You now have a working blog hosted for free, using Quarto and GitHub Pages.\nIf you found this guide helpful, or want to give me your feedback, feel free to connect on LinkedIn or explore more on this blog.\nHappy blogging!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog!",
    "section": "",
    "text": "Data Science Job Postings 2024 (Part I)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Post Title\n\n\n\n\n\n\n\n\nMay 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Download Datasets from Kaggle Using kagglehub\n\n\n\nPython\n\nData\n\nTools\n\n\n\nA step-by-step guide to downloading Kaggle datasets with the new kagglehub Python package.\n\n\n\n\n\nMay 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Blog Using Quarto\n\n\n\ntutorial\n\nquarto\n\ngithub\n\nblogging\n\n\n\nLearn how to set up your own stylish data blog using Quarto and GitHub Pages ‚Äî no advanced web skills required.\n\n\n\n\n\nMay 11, 2025\n\n\nFlavia Felletti\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! Here is where I share my journey in data science through hands-on projects, tutorials, and reflections. I hope you‚Äôll find something useful, inspiring, or entertaining for you here."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! Here is where I share my journey in data science through hands-on projects, tutorials, and reflections. I hope you‚Äôll find something useful, inspiring, or entertaining for you here."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "About Me",
    "text": "About Me\nMy name is Flavia and I am currently a data scientist based in Hamburg, Germany. I also have a PhD in philosophy and research experience in empirical psychology. Both taught me useful skills that now I like to apply outside academia. I‚Äôm also a Brazilian Zouk dance teacher and event organizer. If you are curious about my dance life, you can visit Share Zouk Hamburg.\nThis blog brings together my curiosity, creativity, and desire to share knowledge. Thanks for visiting!"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html",
    "href": "posts/job_postings_analysis_part1.html",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "A hands-on walkthrough using a LinkedIn-based dataset to clean and explore job titles, companies, and locations.\n\n\n\n\nThis notebook uses the Data Science Job Postings & Skills (2024) dataset, authored by asaniczka, which contains real job listings scraped from LinkedIn. You can download it in two ways:\n\n\n\nVisit the dataset page here:\nüëâ Kaggle - Data Science Job Postings and Skills\n\n\n\n\nIf you‚Äôre using Python, follow my step-by-step guide to downloading Kaggle datasets using the new kagglehub library:\nüëâ How to Download Datasets from Kaggle Using kagglehub\n\nOnce you‚Äôve downloaded the dataset, make sure the following CSV files are in your working directory:\n\njob_posting.csv\njob_skills.csv\njob_summary.csv\n\n\n\n\n\n\nThis notebook is part of a multi-post series analyzing a dataset of real Data Science job postings collected from LinkedIn in 2024.\nIn this first part, we focus on exploring the structure of the job_posting.csv file and performing basic cleaning and exploratory analysis.\nColumns overview:\n\njob_link: direct link to the job posting\njob_title: job title\ncompany: company name\njob_location: job location (city/state/country)\nfirst_seen: when the job was first scraped\nAdditional metadata: processing status flags\n\nOur goal today is to clean the data and explore job titles, companies, and job locations.\nLoad the necessary libraries and (optional) set the preferred configurations for your data visualizations.\n\n# load the necessary libraries and configure the settings for data visualizations\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# viz. configurations\nsns.set(style=\"whitegrid\") # improves plots readability\nplt.rcParams[\"figure.figsize\"] = (10, 5) # this configuration is best suitable for blog posts like this\n\nLoad your data:\n\ndf = pd.read_csv(\"job_postings.csv\")\ndf.head(3)\n\n\n\n\n\n\n\n\njob_link\nlast_processed_time\nlast_status\ngot_summary\ngot_ner\nis_being_worked\njob_title\ncompany\njob_location\nfirst_seen\nsearch_city\nsearch_country\nsearch_position\njob_level\njob_type\n\n\n\n\n0\nhttps://www.linkedin.com/jobs/view/senior-mach...\n2024-01-21 08:08:48.031964+00\nFinished NER\nt\nt\nf\nSenior Machine Learning Engineer\nJobs for Humanity\nNew Haven, CT\n2024-01-14\nEast Haven\nUnited States\nAgricultural-Research Engineer\nMid senior\nOnsite\n\n\n1\nhttps://www.linkedin.com/jobs/view/principal-s...\n2024-01-20 04:02:12.331406+00\nFinished NER\nt\nt\nf\nPrincipal Software Engineer, ML Accelerators\nAurora\nSan Francisco, CA\n2024-01-14\nEl Cerrito\nUnited States\nSet-Key Driver\nMid senior\nOnsite\n\n\n2\nhttps://www.linkedin.com/jobs/view/senior-etl-...\n2024-01-21 08:08:31.941595+00\nFinished NER\nt\nt\nf\nSenior ETL Data Warehouse Specialist\nAdame Services LLC\nNew York, NY\n2024-01-14\nMiddletown\nUnited States\nTechnical Support Specialist\nAssociate\nOnsite\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12217 entries, 0 to 12216\nData columns (total 15 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   job_link             12217 non-null  object\n 1   last_processed_time  12217 non-null  object\n 2   last_status          12217 non-null  object\n 3   got_summary          12217 non-null  object\n 4   got_ner              12217 non-null  object\n 5   is_being_worked      12217 non-null  object\n 6   job_title            12217 non-null  object\n 7   company              12217 non-null  object\n 8   job_location         12216 non-null  object\n 9   first_seen           12217 non-null  object\n 10  search_city          12217 non-null  object\n 11  search_country       12217 non-null  object\n 12  search_position      12217 non-null  object\n 13  job_level            12217 non-null  object\n 14  job_type             12217 non-null  object\ndtypes: object(15)\nmemory usage: 1.4+ MB\n\n\n\n\n\nAs we will not use all the columns, we will now drop the unnecessary ones, reducing thus the volume of the dataset:\n\n# drop unnecessary columns\ndf_cleaned = df.drop(columns=[\n    'job_link', 'last_processed_time', 'last_status', \n    'got_summary', 'got_ner', 'is_being_worked'\n])\n\n\n\n\nLet‚Äôs now clean our dataset from duplicate values and look for missing values.\n\n# drop duplicates\ndf_cleaned = df_cleaned.drop_duplicates()\n\n\n# count missing values\ndf_cleaned.isnull().sum()\n\njob_title          0\ncompany            0\njob_location       0\nfirst_seen         0\nsearch_city        0\nsearch_country     0\nsearch_position    0\njob_level          0\njob_type           0\ndtype: int64\n\n\nWe found one missing value in job_location. This time we will just remove the missing row with teh missing value. In other contexts, it makes sense to deal with misisng values in other ways. You can read about methods to deal with missing values here: https://www.geeksforgeeks.org/ml-handling-missing-values/.\n\n# drop missing values\ndf_cleaned = df_cleaned.dropna()\n\n\n\n\nWe will now have a deeper look at teh dataset, in particular focusing on:\n\njob titles\ncompanies\nlocation\n\nLet‚Äôs count the number of entries for each job title, company and location:\n\n# count entriep per job title, diplay the first 10\ndf_cleaned['job_title'].value_counts().head(10)\n\njob_title\nSenior Data Engineer                         273\nSenior Data Analyst                          157\nData Engineer                                146\nSenior MLOps Engineer                        138\nData Analyst                                 131\nData Scientist                               127\nSenior Data Scientist                        117\nLead Data Engineer                           116\nData Architect                               106\nStaff Machine Learning Engineer, Series A     99\nName: count, dtype: int64\n\n\n\n# count the entries for each company\ndf_cleaned['company'].value_counts().head(10)\n\ncompany\nJobs for Humanity            669\nRecruiting from Scratch      387\nDice                         179\nAgoda                        172\nClearanceJobs                159\nClickJobs.io                 152\nCapital One                   80\nEnergy Jobline                70\nDeloitte                      67\nAmazon Web Services (AWS)     64\nName: count, dtype: int64\n\n\n\n# count teh entries for each location\ndf_cleaned['job_location'].value_counts().head(10)\n\njob_location\nNew York, NY                       273\nChicago, IL                        232\nLondon, England, United Kingdom    230\nSan Francisco, CA                  196\nWashington, DC                     171\nAustin, TX                         165\nSeattle, WA                        160\nDallas, TX                         154\nBoston, MA                         150\nAtlanta, GA                        144\nName: count, dtype: int64\n\n\nWe can now visualize the results:\n\n# viz. job titles\ntop_titles = df_cleaned['job_title'].value_counts().head(10)\nsns.barplot(x=top_titles.values, y=top_titles.index, hue=top_titles.index, palette=\"viridis\", legend=False)\nplt.title(\"Top 10 Job Titles\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Job Title\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# viz. top 10 hiring companies\ntop_companies = df_cleaned['company'].value_counts().head(10)\nsns.barplot(x=top_companies.values, y=top_companies.index, hue=top_companies.index, palette=\"magma\", legend=False)\nplt.title(\"Top 10 Hiring Companies\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Company\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# top 10 job locations\ntop_locations = df_cleaned['job_location'].value_counts().head(10)\nsns.barplot(x=top_locations.values, y=top_locations.index, palette=\"cubehelix\", hue=top_locations.index, legend=False)\nplt.title(\"Top 10 Job Locations\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Location\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe have seen that:\n\nThe Top 3 Job Titles in data Science are: Senior Data Engineer, Senrios Data Analyst, and Data Engineer.\nThe top 3 Companies are: Jobs For Humanity, Recruiting from Scratch, and Dice.\nThe top 3 Locations are: New York, Chicago, and London.\n\nThis analysis has its clear limitation, and may be highly biased by the language of the job search, i.e., English. So, be carefull when drawing conclusions from these data!\n\n\n\nIn the next post, we‚Äôll explore the job_skills.csv file to identify:\n\nThe most frequently required skills\nSkill trends by job title or level\nKeywords and technologies in demand for Data Scientists\n\nüìå Stay tuned!"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#data-preparation-explanatory-data-analysis",
    "href": "posts/job_postings_analysis_part1.html#data-preparation-explanatory-data-analysis",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "A hands-on walkthrough using a LinkedIn-based dataset to clean and explore job titles, companies, and locations."
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#how-to-get-the-dataset",
    "href": "posts/job_postings_analysis_part1.html#how-to-get-the-dataset",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "This notebook uses the Data Science Job Postings & Skills (2024) dataset, authored by asaniczka, which contains real job listings scraped from LinkedIn. You can download it in two ways:\n\n\n\nVisit the dataset page here:\nüëâ Kaggle - Data Science Job Postings and Skills\n\n\n\n\nIf you‚Äôre using Python, follow my step-by-step guide to downloading Kaggle datasets using the new kagglehub library:\nüëâ How to Download Datasets from Kaggle Using kagglehub\n\nOnce you‚Äôve downloaded the dataset, make sure the following CSV files are in your working directory:\n\njob_posting.csv\njob_skills.csv\njob_summary.csv"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#dataset-overview",
    "href": "posts/job_postings_analysis_part1.html#dataset-overview",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "This notebook is part of a multi-post series analyzing a dataset of real Data Science job postings collected from LinkedIn in 2024.\nIn this first part, we focus on exploring the structure of the job_posting.csv file and performing basic cleaning and exploratory analysis.\nColumns overview:\n\njob_link: direct link to the job posting\njob_title: job title\ncompany: company name\njob_location: job location (city/state/country)\nfirst_seen: when the job was first scraped\nAdditional metadata: processing status flags\n\nOur goal today is to clean the data and explore job titles, companies, and job locations.\nLoad the necessary libraries and (optional) set the preferred configurations for your data visualizations.\n\n# load the necessary libraries and configure the settings for data visualizations\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# viz. configurations\nsns.set(style=\"whitegrid\") # improves plots readability\nplt.rcParams[\"figure.figsize\"] = (10, 5) # this configuration is best suitable for blog posts like this\n\nLoad your data:\n\ndf = pd.read_csv(\"job_postings.csv\")\ndf.head(3)\n\n\n\n\n\n\n\n\njob_link\nlast_processed_time\nlast_status\ngot_summary\ngot_ner\nis_being_worked\njob_title\ncompany\njob_location\nfirst_seen\nsearch_city\nsearch_country\nsearch_position\njob_level\njob_type\n\n\n\n\n0\nhttps://www.linkedin.com/jobs/view/senior-mach...\n2024-01-21 08:08:48.031964+00\nFinished NER\nt\nt\nf\nSenior Machine Learning Engineer\nJobs for Humanity\nNew Haven, CT\n2024-01-14\nEast Haven\nUnited States\nAgricultural-Research Engineer\nMid senior\nOnsite\n\n\n1\nhttps://www.linkedin.com/jobs/view/principal-s...\n2024-01-20 04:02:12.331406+00\nFinished NER\nt\nt\nf\nPrincipal Software Engineer, ML Accelerators\nAurora\nSan Francisco, CA\n2024-01-14\nEl Cerrito\nUnited States\nSet-Key Driver\nMid senior\nOnsite\n\n\n2\nhttps://www.linkedin.com/jobs/view/senior-etl-...\n2024-01-21 08:08:31.941595+00\nFinished NER\nt\nt\nf\nSenior ETL Data Warehouse Specialist\nAdame Services LLC\nNew York, NY\n2024-01-14\nMiddletown\nUnited States\nTechnical Support Specialist\nAssociate\nOnsite\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12217 entries, 0 to 12216\nData columns (total 15 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   job_link             12217 non-null  object\n 1   last_processed_time  12217 non-null  object\n 2   last_status          12217 non-null  object\n 3   got_summary          12217 non-null  object\n 4   got_ner              12217 non-null  object\n 5   is_being_worked      12217 non-null  object\n 6   job_title            12217 non-null  object\n 7   company              12217 non-null  object\n 8   job_location         12216 non-null  object\n 9   first_seen           12217 non-null  object\n 10  search_city          12217 non-null  object\n 11  search_country       12217 non-null  object\n 12  search_position      12217 non-null  object\n 13  job_level            12217 non-null  object\n 14  job_type             12217 non-null  object\ndtypes: object(15)\nmemory usage: 1.4+ MB"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#step-1-drop-irrelevant-columns",
    "href": "posts/job_postings_analysis_part1.html#step-1-drop-irrelevant-columns",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "As we will not use all the columns, we will now drop the unnecessary ones, reducing thus the volume of the dataset:\n\n# drop unnecessary columns\ndf_cleaned = df.drop(columns=[\n    'job_link', 'last_processed_time', 'last_status', \n    'got_summary', 'got_ner', 'is_being_worked'\n])"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#step-2-clean-duplicates-and-missing-values",
    "href": "posts/job_postings_analysis_part1.html#step-2-clean-duplicates-and-missing-values",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "Let‚Äôs now clean our dataset from duplicate values and look for missing values.\n\n# drop duplicates\ndf_cleaned = df_cleaned.drop_duplicates()\n\n\n# count missing values\ndf_cleaned.isnull().sum()\n\njob_title          0\ncompany            0\njob_location       0\nfirst_seen         0\nsearch_city        0\nsearch_country     0\nsearch_position    0\njob_level          0\njob_type           0\ndtype: int64\n\n\nWe found one missing value in job_location. This time we will just remove the missing row with teh missing value. In other contexts, it makes sense to deal with misisng values in other ways. You can read about methods to deal with missing values here: https://www.geeksforgeeks.org/ml-handling-missing-values/.\n\n# drop missing values\ndf_cleaned = df_cleaned.dropna()"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#step-3-explore-the-data",
    "href": "posts/job_postings_analysis_part1.html#step-3-explore-the-data",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "We will now have a deeper look at teh dataset, in particular focusing on:\n\njob titles\ncompanies\nlocation\n\nLet‚Äôs count the number of entries for each job title, company and location:\n\n# count entriep per job title, diplay the first 10\ndf_cleaned['job_title'].value_counts().head(10)\n\njob_title\nSenior Data Engineer                         273\nSenior Data Analyst                          157\nData Engineer                                146\nSenior MLOps Engineer                        138\nData Analyst                                 131\nData Scientist                               127\nSenior Data Scientist                        117\nLead Data Engineer                           116\nData Architect                               106\nStaff Machine Learning Engineer, Series A     99\nName: count, dtype: int64\n\n\n\n# count the entries for each company\ndf_cleaned['company'].value_counts().head(10)\n\ncompany\nJobs for Humanity            669\nRecruiting from Scratch      387\nDice                         179\nAgoda                        172\nClearanceJobs                159\nClickJobs.io                 152\nCapital One                   80\nEnergy Jobline                70\nDeloitte                      67\nAmazon Web Services (AWS)     64\nName: count, dtype: int64\n\n\n\n# count teh entries for each location\ndf_cleaned['job_location'].value_counts().head(10)\n\njob_location\nNew York, NY                       273\nChicago, IL                        232\nLondon, England, United Kingdom    230\nSan Francisco, CA                  196\nWashington, DC                     171\nAustin, TX                         165\nSeattle, WA                        160\nDallas, TX                         154\nBoston, MA                         150\nAtlanta, GA                        144\nName: count, dtype: int64\n\n\nWe can now visualize the results:\n\n# viz. job titles\ntop_titles = df_cleaned['job_title'].value_counts().head(10)\nsns.barplot(x=top_titles.values, y=top_titles.index, hue=top_titles.index, palette=\"viridis\", legend=False)\nplt.title(\"Top 10 Job Titles\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Job Title\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# viz. top 10 hiring companies\ntop_companies = df_cleaned['company'].value_counts().head(10)\nsns.barplot(x=top_companies.values, y=top_companies.index, hue=top_companies.index, palette=\"magma\", legend=False)\nplt.title(\"Top 10 Hiring Companies\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Company\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# top 10 job locations\ntop_locations = df_cleaned['job_location'].value_counts().head(10)\nsns.barplot(x=top_locations.values, y=top_locations.index, palette=\"cubehelix\", hue=top_locations.index, legend=False)\nplt.title(\"Top 10 Job Locations\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Location\")\nplt.show()"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#main-takeouts",
    "href": "posts/job_postings_analysis_part1.html#main-takeouts",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "We have seen that:\n\nThe Top 3 Job Titles in data Science are: Senior Data Engineer, Senrios Data Analyst, and Data Engineer.\nThe top 3 Companies are: Jobs For Humanity, Recruiting from Scratch, and Dice.\nThe top 3 Locations are: New York, Chicago, and London.\n\nThis analysis has its clear limitation, and may be highly biased by the language of the job search, i.e., English. So, be carefull when drawing conclusions from these data!"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#next-steps",
    "href": "posts/job_postings_analysis_part1.html#next-steps",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "In the next post, we‚Äôll explore the job_skills.csv file to identify:\n\nThe most frequently required skills\nSkill trends by job title or level\nKeywords and technologies in demand for Data Scientists\n\nüìå Stay tuned!"
  }
]