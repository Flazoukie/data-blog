[
  {
    "objectID": "posts/kagglehub-download.html",
    "href": "posts/kagglehub-download.html",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "",
    "text": "kagglehub is a relatively new and more user-friendly package developed by Kaggle to simplify access to datasets, notebooks, and models. The kagglehub Python package offers a cleaner alternative.\nThis guide shows you how to download Kaggle datasets in just a few lines of code."
  },
  {
    "objectID": "posts/kagglehub-download.html#what-is-kagglehub",
    "href": "posts/kagglehub-download.html#what-is-kagglehub",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "What is kagglehub?",
    "text": "What is kagglehub?\nkagglehub is a Python package developed by Kaggle that allows you to programmatically download datasets, notebooks, and models."
  },
  {
    "objectID": "posts/kagglehub-download.html#step-1-install-kagglehub",
    "href": "posts/kagglehub-download.html#step-1-install-kagglehub",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Step 1: Install kagglehub",
    "text": "Step 1: Install kagglehub\npip install kagglehub"
  },
  {
    "objectID": "posts/kagglehub-download.html#step-2-authenticate-with-kaggle",
    "href": "posts/kagglehub-download.html#step-2-authenticate-with-kaggle",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Step 2: Authenticate with Kaggle",
    "text": "Step 2: Authenticate with Kaggle\nTo use kagglehub, you need a Kaggle API token.\n\nOption A: Save the token file\nGo to your Kaggle account settings: https://www.kaggle.com/account\nScroll to ‚ÄúAPI‚Äù and click ‚ÄúCreate New API Token‚Äù\nSave the downloaded kaggle.json file to:\nLinux/macOS: ~/.kaggle/kaggle.json\nWindows: C:&lt;YourUsername&gt;.kaggle.json\nIf the .kaggle folder doesn‚Äôt exist, create it manually.\n\n\nOption B: Use environment variables\nimport os\n\nos.environ[\"KAGGLE_USERNAME\"] = \"your_username\"\nos.environ[\"KAGGLE_KEY\"] = \"your_key\"\nReplace ‚Äúyour_username‚Äù and ‚Äúyour_key‚Äù with your API credentials from kaggle.json."
  },
  {
    "objectID": "posts/kagglehub-download.html#step-3-download-a-dataset",
    "href": "posts/kagglehub-download.html#step-3-download-a-dataset",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Step 3: Download a Dataset",
    "text": "Step 3: Download a Dataset\nHere‚Äôs how to download your dataset:\nimport kagglehub\n\npath = kagglehub.dataset_download(\"yourdataset\")\n\nprint(\"Dataset downloaded to:\", path)\nThe dataset is automatically downloaded and unzipped into a cache directory (usually ~/.cache/kagglehub/)."
  },
  {
    "objectID": "posts/kagglehub-download.html#step-4-explore-the-files",
    "href": "posts/kagglehub-download.html#step-4-explore-the-files",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Step 4: Explore the Files",
    "text": "Step 4: Explore the Files\nimport os\n\nfor file in os.listdir(path):\n    print(file)"
  },
  {
    "objectID": "posts/kagglehub-download.html#bonus-reusability-and-automation",
    "href": "posts/kagglehub-download.html#bonus-reusability-and-automation",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Bonus: Reusability and Automation",
    "text": "Bonus: Reusability and Automation\n\nkagglehub also works for notebooks and models.\nFiles are cached and won‚Äôt be redownloaded unless updated.\nGreat for use in automated workflows or reproducible projects."
  },
  {
    "objectID": "posts/kagglehub-download.html#conclusion",
    "href": "posts/kagglehub-download.html#conclusion",
    "title": "How to Download Datasets from Kaggle Using kagglehub",
    "section": "Conclusion",
    "text": "Conclusion\nkagglehub is a simple, effective alternative to the Kaggle CLI, especially for Python users who prefer to keep everything in code. With just a few lines, you can grab datasets and start analyzing!"
  },
  {
    "objectID": "posts/how-to-create-a-blog.html",
    "href": "posts/how-to-create-a-blog.html",
    "title": "How to Create a Blog Using Quarto",
    "section": "",
    "text": "Creating your own blog is a great way to share your data projects. With Quarto and GitHub Pages, you can easily publish a professional-looking blog for free, using simple Markdown and code.\nThis setup is ideal for:\n\nShowcasing data science or programming projects\nWriting technical tutorials (like this ;) )\nMaintaining a lightweight, fast, and version-controlled website\n\nIn this tutorial, I‚Äôll walk you through how to create your blog step by step using Quarto and GitHub Pages."
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#introduction",
    "href": "posts/how-to-create-a-blog.html#introduction",
    "title": "How to Create a Blog Using Quarto",
    "section": "",
    "text": "Creating your own blog is a great way to share your data projects. With Quarto and GitHub Pages, you can easily publish a professional-looking blog for free, using simple Markdown and code.\nThis setup is ideal for:\n\nShowcasing data science or programming projects\nWriting technical tutorials (like this ;) )\nMaintaining a lightweight, fast, and version-controlled website\n\nIn this tutorial, I‚Äôll walk you through how to create your blog step by step using Quarto and GitHub Pages."
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#what-youll-need",
    "href": "posts/how-to-create-a-blog.html#what-youll-need",
    "title": "How to Create a Blog Using Quarto",
    "section": "üß∞ What You‚Äôll Need",
    "text": "üß∞ What You‚Äôll Need\n\nTechnical Requirements\n\nInstall Quarto Quarto is the publishing engine. You can write in Markdown, embed code, and produce beautiful web pages, one great thing about Quarto is that it supports various languages like R, Python, Julia, and Observable JavaScript. Download and install it from the official site.\nCreate a GitHub account (if you don‚Äôt have one yet) You‚Äôll use GitHub to host your website for free via GitHub Pages.\nInstall Git (if not already installed) Quarto uses Git to publish your site to GitHub. Install Git and set it up locally.\n\n\nOptional but helpful: A code editor like VS Code or PyCharm makes writing .qmd files easier, but it‚Äôs not required. I am personally using PyCharm Community Edition (the free version of PyCharm) Download PyCharm."
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#step-by-step-create-your-blog",
    "href": "posts/how-to-create-a-blog.html#step-by-step-create-your-blog",
    "title": "How to Create a Blog Using Quarto",
    "section": "üöÄ Step-by-Step: Create Your Blog",
    "text": "üöÄ Step-by-Step: Create Your Blog\nWe‚Äôll now create and publish your blog in just a few steps.\n\n\n1. Create a New Quarto Website\nOpen your terminal and run the following command:\nquarto create-project my-blog --type website\ncd my-blog\nThis creates a new folder called ‚Äúmy-blog‚Äù with the basic structure of a Quarto website. You can rename my-blog to whatever you want. You should now have gotten the following files:\n\nindex.qmd (your homepage)\nabout.qmd (your About page)\n_quarto.yml (the configuration file that defines your site layout and style)\nA posts/ folder to hold your blog posts\n\n\nIf you didn‚Äôt get a posts/ folder, create it manually. You will need it to hold all your blog posts!\n\n\n\n2. Preview Your Website Locally\nTo check if everything worked correctly, run the following command to see your site in your browser:\nquarto preview\nThis launches a localhost preview. Any time you change a .qmd file and save it, the preview updates automatically. Remember, this is not your site yet, but just a local preview. Next I will guide you through creating a proper site using GitHub Pages.\n\n\n3. Configure the Site for GitHub Pages\nTo publish your site using GitHub Pages, you need to configure Quarto to render your output into a folder named docs/.\nIn the root of your project, open the _quarto.yml file and modify or add the following line:\nproject:\n  type: website\n  output-dir: docs\nThen render your site:\nquarto render\nQuarto will generate the site in a docs/ folder, which GitHub Pages can use for publishing.\n\n\n4. Push Your Site to GitHub\nCreate a new repository on GitHub (e.g., ‚Äúmy-blog‚Äù)\nInitialize Git and link your local project to your GitHub repo:\ngit init\ngit remote add origin https://github.com/yourusername/my-blog.git\ngit add .\ngit commit -m \"Initial commit\"\ngit push -u origin main\nDon‚Äôt forget to replace ‚Äúyourusername‚Äù with your actual username and ‚Äúmy-blog‚Äù with your actual blog name ;). üîê If you‚Äôre using HTTPS, GitHub will ask you for a token instead of a password. You can generate one here.\n\n\n5. Enable GitHub Pages\n\nGo to your GitHub repository\nClick on Settings ‚Üí Pages\nUnder Source, select:\n\n\nBranch: main\nFolder: /docs\n\nClick Save. After a few seconds, your site will be live at: https://yourusername.github.io/data-blog/\n\nNote: This is the path I chose to publish my blog, however there are alternative ways to do it, which may better satisfy your requirements, have a look here: GitHub Pages for Quarto."
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#create-your-first-blog-post",
    "href": "posts/how-to-create-a-blog.html#create-your-first-blog-post",
    "title": "How to Create a Blog Using Quarto",
    "section": "‚úçüèø Create Your First Blog Post",
    "text": "‚úçüèø Create Your First Blog Post\nIf you do not have a folder for your posts already, create a new one. I keep my posts inside the folder ‚Äúposts‚Äù, within my own project folder DataBlog. Now you can create a new file, which will become your first post. For example:\nposts/how-to-create-a-data-blog.qmd\nYou can start with this YAML header and modify it accordingly with your needs:\n---\ntitle: \"My first post title\"\ndate: 2025-05-11\ncategories: [tutorial, quarto, github, blogging]\nformat:\n  html:\n    toc: true\n    code-tools: true\n    theme: cosmo\n---\nThen write your content below using Markdown and code blocks as needed.\nWhen ready, render the site again:\nquarto render\nNow push the changes to GitHub:\ngit add .\ngit commit -m \"Add first blog post\"\ngit push\nVisit your blog again ‚Äî Congratulations, your new post is live! ‚úåüèø"
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#remove-the-demo-post",
    "href": "posts/how-to-create-a-blog.html#remove-the-demo-post",
    "title": "How to Create a Blog Using Quarto",
    "section": "üóëÔ∏è Remove the Demo Post",
    "text": "üóëÔ∏è Remove the Demo Post\nWhen you create a new Quarto website project, it often comes with a demo blog post inside the posts/ folder. This is just a placeholder to show you how blog posts work. It usually has a name like:\nposts/welcome/index.qmd\n‚ÄúYou‚Äôll probably want to remove the demo post to keep your site clean. I removed them manually from Pycharm, but you can removed them from your terminal using a command like:\nrm posts/welcome/index.qmd\nDon‚Äôt forget not to render and commit!\nquarto render\ngit add .\ngit commit -m \"Remove demo post\"\ngit push"
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#customize-the-site",
    "href": "posts/how-to-create-a-blog.html#customize-the-site",
    "title": "How to Create a Blog Using Quarto",
    "section": "üé® Customize the Site",
    "text": "üé® Customize the Site\nNow you may want to customize the site. To do this, you need to change the _quarto.yml file. Open it and change title, navigation bar, theme, and styling according to your wishes.\nFor example:\nproject:\n  type: website\n  output-dir: docs\n\nwebsite:\n  title: \"My Blog\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\nüß© Optional: Add an Image and About Info\nWrite a small biography in about.qmd. and add a personal image. Here you can also add your contacts.\nThis is how my about.qmd looks like:\n---\ntitle: \"About\"\nimage: flavia_cartoon.png\nabout:\n  template: jolla\n  links:\n    - icon: linkedin\n      text: LinkedIn\n      href: https://www.linkedin.com/in/flavia-felletti-phd-8066141b8/\n    - icon: github\n      text: Github\n      href: https://github.com/Flazoukie\n\n---\n\n## About This Blog\n\nWelcome to my blog! Here is where I share my journey in data science through hands-on projects, tutorials, and reflections.\nI hope you'll find something useful, inspiring, or entertaining for you here.\nFeel free to take inspiration!\nEnsure the image is copied into the docs/ folder so it shows up on GitHub Pages. Remember now to render to turn your Markdown files into a working website:\nquarto render\nThis generates a docs/ folder with all the HTML files that make up your website.\nDon‚Äôt forget to commit and push to GitHub to apply teh changes to your website!\ngit add .\ngit commit -m \"Customize the website\"\ngit push"
  },
  {
    "objectID": "posts/how-to-create-a-blog.html#wrap-up",
    "href": "posts/how-to-create-a-blog.html#wrap-up",
    "title": "How to Create a Blog Using Quarto",
    "section": "‚úÖ Wrap-Up",
    "text": "‚úÖ Wrap-Up\nCongratulations! üéâ You now have a working blog hosted for free, using Quarto and GitHub Pages.\nIf you found this guide helpful, or want to give me your feedback, feel free to connect on LinkedIn or explore more on this blog.\nHappy blogging!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog!",
    "section": "",
    "text": "Data Science Job Postings 2024 (Part II): What Skills Are in Demand?\n\n\n\nData Science\n\nEDA\n\nSkills\n\nJob Market\n\n\n\nAnalyzing required skills from LinkedIn job postings to uncover the most sought-after tools and technologies for data professionals.\n\n\n\n\n\nMay 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Job Postings 2024 (Part I)\n\n\n\nData Science\n\nEDA\n\nJob Market\n\n\n\nA hands-on walkthrough using a LinkedIn-based dataset to clean and explore job titles, companies, and locations.\n\n\n\n\n\nMay 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Download Datasets from Kaggle Using kagglehub\n\n\n\nPython\n\nData\n\nTools\n\n\n\nA step-by-step guide to downloading Kaggle datasets with the new kagglehub Python package.\n\n\n\n\n\nMay 15, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Blog Using Quarto\n\n\n\ntutorial\n\nquarto\n\ngithub\n\nblogging\n\n\n\nLearn how to set up your own stylish data blog using Quarto and GitHub Pages ‚Äî no advanced web skills required.\n\n\n\n\n\nMay 11, 2025\n\n\nFlavia Felletti\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! Here is where I share my journey in data science through hands-on projects, tutorials, and reflections. I hope you‚Äôll find something useful, inspiring, or entertaining for you here."
  },
  {
    "objectID": "about.html#about-this-blog",
    "href": "about.html#about-this-blog",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog! Here is where I share my journey in data science through hands-on projects, tutorials, and reflections. I hope you‚Äôll find something useful, inspiring, or entertaining for you here."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "About Me",
    "text": "About Me\nMy name is Flavia and I am currently a data scientist based in Hamburg, Germany. I also have a PhD in philosophy and research experience in empirical psychology. Both taught me useful skills that now I like to apply outside academia. I‚Äôm also a Brazilian Zouk dance teacher and event organizer. If you are curious about my dance life, you can visit Share Zouk Hamburg.\nThis blog brings together my curiosity, creativity, and desire to share knowledge. Thanks for visiting!"
  },
  {
    "objectID": "posts/2024-data-science-jobs-part2.html",
    "href": "posts/2024-data-science-jobs-part2.html",
    "title": "Data Science Job Postings 2024 (Part II): What Skills Are in Demand?",
    "section": "",
    "text": "In this second part of our analysis of the Data Science Job Postings 2024 dataset, we shift our focus to the skills that employers are looking for.\nWe‚Äôll explore the job_skills.csv file that we downloaded from Kaggle here and: - Analyze the most frequently mentioned skills. - See how skills vary by job title or company. - Visualize the results for a more catchy presentation.\nIf you missed the first part of this series, check it out here.\n\n\nFirst, let‚Äôs load the libraries, the data, and set our configurations. We will also import the previously used dataset, job_postings.csv, as it contains the job titles associated with the skills.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport textwrap\nfrom collections import Counter\nimport altair as alt\n\n# set configurations for the visualizations\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (10, 5)\n\n# load the data\ndf_skills = pd.read_csv(\"job_skills.csv\")\ndf_postings = pd.read_csv('job_postings.csv') # used later to retrieve job titles\n\nwe will merge the two dataframes to be able to retrieve the job titles and avoid extracting it from the job links.\n\n# Merge on job_link\ndf_merged = pd.merge(df_skills, df_postings[['job_link', 'job_title']], on='job_link', how='left')\ndf_merged.head(3)\n\n\n\n\n\n\n\n\njob_link\njob_skills\njob_title\n\n\n\n\n0\nhttps://www.linkedin.com/jobs/view/senior-mach...\nMachine Learning, Programming, Python, Scala, ...\nSenior Machine Learning Engineer\n\n\n1\nhttps://www.linkedin.com/jobs/view/principal-s...\nC++, Python, PyTorch, TensorFlow, MXNet, CUDA,...\nPrincipal Software Engineer, ML Accelerators\n\n\n2\nhttps://www.linkedin.com/jobs/view/senior-etl-...\nETL, Data Integration, Data Transformation, Da...\nSenior ETL Data Warehouse Specialist\n\n\n\n\n\n\n\n\n\n\nThose functions have been used throughout the notebook:\n\n# clean and format the skills\ndef clean_skills(skills_series):\n    \"\"\"\n    Clean raw skill strings: explode, strip whitespace, and lowercase.\n    Returns a Series of individual cleaned skills.\n    \"\"\"\n    skills = skills_series.dropna().str.split(',').explode()\n    return skills.str.strip().str.lower()\n\n# merged redundant skills\ndef apply_skill_mapping(cleaned_skills, skill_mapping):\n    \"\"\"\n    Apply skill mapping to merge redundant skills.\n    Returns a cleaned and consolidated Series with title case.\n    \"\"\"\n    skills_mapped = cleaned_skills.replace(skill_mapping)\n    return skills_mapped.str.title()"
  },
  {
    "objectID": "posts/2024-data-science-jobs-part2.html#introduction",
    "href": "posts/2024-data-science-jobs-part2.html#introduction",
    "title": "Data Science Job Postings 2024 (Part II): What Skills Are in Demand?",
    "section": "",
    "text": "In this second part of our analysis of the Data Science Job Postings 2024 dataset, we shift our focus to the skills that employers are looking for.\nWe‚Äôll explore the job_skills.csv file that we downloaded from Kaggle here and: - Analyze the most frequently mentioned skills. - See how skills vary by job title or company. - Visualize the results for a more catchy presentation.\nIf you missed the first part of this series, check it out here.\n\n\nFirst, let‚Äôs load the libraries, the data, and set our configurations. We will also import the previously used dataset, job_postings.csv, as it contains the job titles associated with the skills.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport textwrap\nfrom collections import Counter\nimport altair as alt\n\n# set configurations for the visualizations\nsns.set(style=\"whitegrid\")\nplt.rcParams[\"figure.figsize\"] = (10, 5)\n\n# load the data\ndf_skills = pd.read_csv(\"job_skills.csv\")\ndf_postings = pd.read_csv('job_postings.csv') # used later to retrieve job titles\n\nwe will merge the two dataframes to be able to retrieve the job titles and avoid extracting it from the job links.\n\n# Merge on job_link\ndf_merged = pd.merge(df_skills, df_postings[['job_link', 'job_title']], on='job_link', how='left')\ndf_merged.head(3)\n\n\n\n\n\n\n\n\njob_link\njob_skills\njob_title\n\n\n\n\n0\nhttps://www.linkedin.com/jobs/view/senior-mach...\nMachine Learning, Programming, Python, Scala, ...\nSenior Machine Learning Engineer\n\n\n1\nhttps://www.linkedin.com/jobs/view/principal-s...\nC++, Python, PyTorch, TensorFlow, MXNet, CUDA,...\nPrincipal Software Engineer, ML Accelerators\n\n\n2\nhttps://www.linkedin.com/jobs/view/senior-etl-...\nETL, Data Integration, Data Transformation, Da...\nSenior ETL Data Warehouse Specialist\n\n\n\n\n\n\n\n\n\n\nThose functions have been used throughout the notebook:\n\n# clean and format the skills\ndef clean_skills(skills_series):\n    \"\"\"\n    Clean raw skill strings: explode, strip whitespace, and lowercase.\n    Returns a Series of individual cleaned skills.\n    \"\"\"\n    skills = skills_series.dropna().str.split(',').explode()\n    return skills.str.strip().str.lower()\n\n# merged redundant skills\ndef apply_skill_mapping(cleaned_skills, skill_mapping):\n    \"\"\"\n    Apply skill mapping to merge redundant skills.\n    Returns a cleaned and consolidated Series with title case.\n    \"\"\"\n    skills_mapped = cleaned_skills.replace(skill_mapping)\n    return skills_mapped.str.title()"
  },
  {
    "objectID": "posts/2024-data-science-jobs-part2.html#most-frequently-required-skills",
    "href": "posts/2024-data-science-jobs-part2.html#most-frequently-required-skills",
    "title": "Data Science Job Postings 2024 (Part II): What Skills Are in Demand?",
    "section": "üß† Most Frequently Required Skills",
    "text": "üß† Most Frequently Required Skills\nWhich are the skills the most frequent skills in the job postings?\n\nüßπ Initial Cleaning\nWe need to do some cleaning here to be able to extract the skills. The dataset contains long comma-separated strings, where each string lists multiple skills required for a job. We will go through teh following steps:\n\nSplit the string by commas, turning each skill string into a list of skills.\nBecause the split creates a list per row, flatten these lists so that each skill becomes its own row.\nWe remove leading and trailing spaces.\nWe convert everything to lowercase for consistency.\n\n\n# Explode and clean individual skills\nall_skills_cleaned = clean_skills(df_merged['job_skills'])\n\n# View the most common raw cleaned skills\nskill_counts_raw = all_skills_cleaned.value_counts().head(20)\nprint(skill_counts_raw)\n\njob_skills\npython                  4810\nsql                     4610\ndata analysis           3293\nmachine learning        2684\ncommunication           2506\ndata visualization      2330\naws                     1740\nproject management      1737\ndata engineering        1714\ncommunication skills    1712\ntableau                 1705\ndata science            1691\nr                       1544\ndata modeling           1520\ndata management         1470\njava                    1434\nproblem solving         1410\ndata warehousing        1408\ndata analytics          1402\nspark                   1395\nName: count, dtype: int64\n\n\nAs we can see, this list of skills is not fully clean, as some skills repeat under different labels, e.g., ‚Äúcommunication‚Äù and ‚Äúcommunication skills‚Äù.\n\n‚ùóMy recommendation for you would be to go through the full list of skills and clean it up before proceeding!\n\n\n\nüîÅ Apply Mapping to Avoid Redundant Skills\nHere I will just clean up those skills that I noticed being redundant throughout this notebook:\n\n# Mapping of skill variants to canonical names\nskill_mapping = {\n    'communication skills': 'communication',\n    'sql': 'SQL',\n    'python': 'Python',\n    'r': 'R',\n    'power bi': 'Power BI',\n    'aws': 'AWS',\n    'data analysis': 'Data Analysis',\n    'data analytics': 'Data Analysis',\n    'data architect': 'Data Architecture',\n    'data architecture': 'Data Architecture',\n    'tensorflow': 'TensorFlow',\n    'llms': 'LLMs',\n    'nlp': 'NLP',\n}\n\n\n# Apply mapping\nall_skills_mapped = apply_skill_mapping(all_skills_cleaned, skill_mapping)\n\n# Count most common final skills\nskill_counts = all_skills_mapped.value_counts().head(20)\nprint(skill_counts)\n\njob_skills\nPython                4810\nData Analysis         4695\nSql                   4610\nCommunication         4218\nMachine Learning      2684\nData Visualization    2330\nAws                   1740\nProject Management    1737\nData Engineering      1714\nTableau               1705\nData Science          1691\nR                     1544\nData Modeling         1520\nData Management       1470\nJava                  1434\nProblem Solving       1410\nData Warehousing      1408\nSpark                 1395\nTeamwork              1227\nAnalytical Skills     1219\nName: count, dtype: int64\n\n\n\n\nüîù The 20 Most In-Demand Skills in Data Job Postings\nLet‚Äôs plot now the top 20 skills.\nWe also:\n\nInvert the y-axis to have the most common skill at the top.\nAdd counts on the bars to show exact frequencies.\n\n\n# Plot top 20 skills:\nplt.figure(figsize=(14, 8))\nbars = plt.barh(skill_counts.index, skill_counts.values, color='cornflowerblue')\nplt.gca().invert_yaxis()  # Show the most frequent skill at the top\n\n# Add frequency labels to each bar\nfor bar in bars:\n    width = bar.get_width()\n    plt.text(width + 1, bar.get_y() + bar.get_height()/2, str(width), va='center')\n\nplt.title(\"Top 20 In-Demand Skills\", fontsize=16)\nplt.xlabel(\"Frequency\", fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/2024-data-science-jobs-part2.html#top-skills-by-job-title",
    "href": "posts/2024-data-science-jobs-part2.html#top-skills-by-job-title",
    "title": "Data Science Job Postings 2024 (Part II): What Skills Are in Demand?",
    "section": "üîù Top Skills by Job Title",
    "text": "üîù Top Skills by Job Title\nHow do skill requirements vary across different job titles?\nWe will now built an interactive graphic to show for the top 10 in deman jobs the 10 most required skills.\nFirstly we will build a new dataframe with the selected job titles and most required skills for them, and we will count the frequency for each skill. We will use this dataframe to create the graphic.\nWe will do it in the following steps:\n\nExtract top 10 job titles\nGet the top 10 skills for each job title\nCount the frequency of each skill for each job title\n\n\n# Get the top 10 most frequent job titles\ntop_titles = df_merged['job_title'].value_counts().head(10).index.tolist()\n\n\n# Get the top 10 skills for the top 10 job titles\nrows = []\n\nfor title in top_titles:\n    # Get relevant skill strings for this title\n    skills_series = df_merged[df_merged['job_title'] == title]['job_skills']\n    \n    # Clean raw skills\n    cleaned_skills = clean_skills(skills_series)\n\n    # Apply skill mapping to avoid redundancy\n    mapped_skills = apply_skill_mapping(cleaned_skills, skill_mapping)\n    \n    # Count the top 10 skills\n    skill_counts = mapped_skills.value_counts().head(10)\n\n    # Step 4: Append to list\n    for skill, count in skill_counts.items():\n        rows.append({\n            'job_title': title,\n            'job_skill': skill,\n            'count': count\n        })\n\n\n# Convert to DataFrame\ntop_skills_filtered = pd.DataFrame(rows)\ntop_skills_filtered = top_skills_filtered.sort_values(['job_title', 'count'], ascending=[True, False])\n\ntop_skills_filtered.head(3)\n\n\n\n\n\n\n\n\njob_title\njob_skill\ncount\n\n\n\n\n40\nData Analyst\nData Analysis\n126\n\n\n41\nData Analyst\nSql\n89\n\n\n42\nData Analyst\nData Visualization\n73\n\n\n\n\n\n\n\n\nimport altair as alt\n\n# Create a dropdown selection for job title\ntitle_dropdown = alt.binding_select(options=top_skills_filtered['job_title'].unique().tolist(), name='Job Title: ')\nselection = alt.selection_point(fields=['job_title'], bind=title_dropdown)\n\n# Build the chart\nchart = (\n    alt.Chart(top_skills_filtered)\n    .transform_filter(selection)\n    .mark_bar(color='cornflowerblue')\n    .encode(\n        x=alt.X('count:Q', title='Count'),\n        y=alt.Y('job_skill:N', sort='-x', title='Skill'),\n        tooltip=['job_skill', 'count']\n    )\n    .add_params(selection)\n    .properties(\n        title='Top 10 Skills by Job Title',\n        width=600,\n        height=400\n    )\n)\n\nchart"
  },
  {
    "objectID": "posts/2024-data-science-jobs-part2.html#conclusion",
    "href": "posts/2024-data-science-jobs-part2.html#conclusion",
    "title": "Data Science Job Postings 2024 (Part II): What Skills Are in Demand?",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis gives us a clearer picture of the technical expectations for data science professionals in 2024. Python, Data Analysis, SQL, and machine learning are core skills core skills, but don‚Äôt forget that communication, teamwork, problem solving, and other soft skills are also essential!\nAlso, please keep in mind that those results rely on a dataset that has been generated by scraping jobs from LinkedIn, and that it may be biased toward english speaking jobs and the english speaking market. See the Part I of this analysis here to have an overview of the top locations and companies considered in this project.\nIn the next post, we‚Äôll analyze job summaries using NLP techniques to uncover trends in job descriptions."
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html",
    "href": "posts/job_postings_analysis_part1.html",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "A hands-on walkthrough using a LinkedIn-based dataset to clean and explore job titles, companies, and locations.\n\n\n\n\nThis notebook uses the Data Science Job Postings & Skills (2024) dataset, authored by asaniczka, which contains real job listings scraped from LinkedIn. You can download it in two ways:\n\n\n\nVisit the dataset page here:\nüëâ Kaggle - Data Science Job Postings and Skills\n\n\n\n\nIf you‚Äôre using Python, follow my step-by-step guide to downloading Kaggle datasets using the new kagglehub library:\nüëâ How to Download Datasets from Kaggle Using kagglehub\n\nOnce you‚Äôve downloaded the dataset, make sure the following CSV files are in your working directory:\n\njob_postings.csv\njob_skills.csv\njob_summary.csv\n\n\n\n\n\n\nThis notebook is part of a multi-post series analyzing a dataset of real Data Science job postings collected from LinkedIn in 2024.\nIn this first part, we focus on exploring the structure of the job_posting.csv file and performing basic cleaning and exploratory analysis.\nColumns overview:\n\njob_link: direct link to the job posting\njob_title: job title\ncompany: company name\njob_location: job location (city/state/country)\nfirst_seen: when the job was first scraped\nAdditional metadata: processing status flags\n\nOur goal today is to clean the data and explore job titles, companies, and job locations.\nLoad the necessary libraries and (optional) set the preferred configurations for your data visualizations.\n\n# load the necessary libraries and configure the settings for data visualizations\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# viz. configurations\nsns.set(style=\"whitegrid\") # improves plots readability\nplt.rcParams[\"figure.figsize\"] = (10, 5) # this configuration is best suitable for blog posts like this\n\nLoad your data:\n\ndf = pd.read_csv(\"job_postings.csv\")\ndf.head(3)\n\n\n\n\n\n\n\n\njob_link\nlast_processed_time\nlast_status\ngot_summary\ngot_ner\nis_being_worked\njob_title\ncompany\njob_location\nfirst_seen\nsearch_city\nsearch_country\nsearch_position\njob_level\njob_type\n\n\n\n\n0\nhttps://www.linkedin.com/jobs/view/senior-mach...\n2024-01-21 08:08:48.031964+00\nFinished NER\nt\nt\nf\nSenior Machine Learning Engineer\nJobs for Humanity\nNew Haven, CT\n2024-01-14\nEast Haven\nUnited States\nAgricultural-Research Engineer\nMid senior\nOnsite\n\n\n1\nhttps://www.linkedin.com/jobs/view/principal-s...\n2024-01-20 04:02:12.331406+00\nFinished NER\nt\nt\nf\nPrincipal Software Engineer, ML Accelerators\nAurora\nSan Francisco, CA\n2024-01-14\nEl Cerrito\nUnited States\nSet-Key Driver\nMid senior\nOnsite\n\n\n2\nhttps://www.linkedin.com/jobs/view/senior-etl-...\n2024-01-21 08:08:31.941595+00\nFinished NER\nt\nt\nf\nSenior ETL Data Warehouse Specialist\nAdame Services LLC\nNew York, NY\n2024-01-14\nMiddletown\nUnited States\nTechnical Support Specialist\nAssociate\nOnsite\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12217 entries, 0 to 12216\nData columns (total 15 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   job_link             12217 non-null  object\n 1   last_processed_time  12217 non-null  object\n 2   last_status          12217 non-null  object\n 3   got_summary          12217 non-null  object\n 4   got_ner              12217 non-null  object\n 5   is_being_worked      12217 non-null  object\n 6   job_title            12217 non-null  object\n 7   company              12217 non-null  object\n 8   job_location         12216 non-null  object\n 9   first_seen           12217 non-null  object\n 10  search_city          12217 non-null  object\n 11  search_country       12217 non-null  object\n 12  search_position      12217 non-null  object\n 13  job_level            12217 non-null  object\n 14  job_type             12217 non-null  object\ndtypes: object(15)\nmemory usage: 1.4+ MB\n\n\n\n\n\nAs we will not use all the columns, we will now drop the unnecessary ones, reducing thus the volume of the dataset:\n\n# drop unnecessary columns\ndf_cleaned = df.drop(columns=[\n    'job_link', 'last_processed_time', 'last_status', \n    'got_summary', 'got_ner', 'is_being_worked'\n])\n\n\n\n\nLet‚Äôs now clean our dataset from duplicate values and look for missing values.\n\n# drop duplicates\ndf_cleaned = df_cleaned.drop_duplicates()\n\n\n# count missing values\ndf_cleaned.isnull().sum()\n\njob_title          0\ncompany            0\njob_location       0\nfirst_seen         0\nsearch_city        0\nsearch_country     0\nsearch_position    0\njob_level          0\njob_type           0\ndtype: int64\n\n\nWe found one missing value in job_location. This time we will just remove the missing row with teh missing value. In other contexts, it makes sense to deal with misisng values in other ways. You can read about methods to deal with missing values here: https://www.geeksforgeeks.org/ml-handling-missing-values/.\n\n# drop missing values\ndf_cleaned = df_cleaned.dropna()\n\n\n\n\nWe will now have a deeper look at teh dataset, in particular focusing on:\n\njob titles\ncompanies\nlocation\n\nLet‚Äôs count the number of entries for each job title, company and location:\n\n# count entriep per job title, diplay the first 10\ndf_cleaned['job_title'].value_counts().head(10)\n\njob_title\nSenior Data Engineer                         273\nSenior Data Analyst                          157\nData Engineer                                146\nSenior MLOps Engineer                        138\nData Analyst                                 131\nData Scientist                               127\nSenior Data Scientist                        117\nLead Data Engineer                           116\nData Architect                               106\nStaff Machine Learning Engineer, Series A     99\nName: count, dtype: int64\n\n\n\n# count the entries for each company\ndf_cleaned['company'].value_counts().head(10)\n\ncompany\nJobs for Humanity            669\nRecruiting from Scratch      387\nDice                         179\nAgoda                        172\nClearanceJobs                159\nClickJobs.io                 152\nCapital One                   80\nEnergy Jobline                70\nDeloitte                      67\nAmazon Web Services (AWS)     64\nName: count, dtype: int64\n\n\n\n# count teh entries for each location\ndf_cleaned['job_location'].value_counts().head(10)\n\njob_location\nNew York, NY                       273\nChicago, IL                        232\nLondon, England, United Kingdom    230\nSan Francisco, CA                  196\nWashington, DC                     171\nAustin, TX                         165\nSeattle, WA                        160\nDallas, TX                         154\nBoston, MA                         150\nAtlanta, GA                        144\nName: count, dtype: int64\n\n\nWe can now visualize the results:\n\n# viz. job titles\ntop_titles = df_cleaned['job_title'].value_counts().head(10)\nsns.barplot(x=top_titles.values, y=top_titles.index, hue=top_titles.index, palette=\"viridis\", legend=False)\nplt.title(\"Top 10 Job Titles\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Job Title\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# viz. top 10 hiring companies\ntop_companies = df_cleaned['company'].value_counts().head(10)\nsns.barplot(x=top_companies.values, y=top_companies.index, hue=top_companies.index, palette=\"magma\", legend=False)\nplt.title(\"Top 10 Hiring Companies\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Company\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# top 10 job locations\ntop_locations = df_cleaned['job_location'].value_counts().head(10)\nsns.barplot(x=top_locations.values, y=top_locations.index, palette=\"cubehelix\", hue=top_locations.index, legend=False)\nplt.title(\"Top 10 Job Locations\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Location\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe have seen that:\n\nThe Top 3 Job Titles in data Science are: Senior Data Engineer, Senrios Data Analyst, and Data Engineer.\nThe top 3 Companies are: Jobs For Humanity, Recruiting from Scratch, and Dice.\nThe top 3 Locations are: New York, Chicago, and London.\n\nThis analysis has its clear limitation, and may be highly biased by the language of the job search, i.e., English. So, be carefull when drawing conclusions from these data!\n\n\n\nIn the next post, we‚Äôll explore the job_skills.csv file to identify:\n\nThe most frequently required skills\nSkill trends by job title or level\nKeywords and technologies in demand for Data Scientists\n\nüìå Stay tuned!"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#data-preparation-explanatory-data-analysis",
    "href": "posts/job_postings_analysis_part1.html#data-preparation-explanatory-data-analysis",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "A hands-on walkthrough using a LinkedIn-based dataset to clean and explore job titles, companies, and locations."
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#how-to-get-the-dataset",
    "href": "posts/job_postings_analysis_part1.html#how-to-get-the-dataset",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "This notebook uses the Data Science Job Postings & Skills (2024) dataset, authored by asaniczka, which contains real job listings scraped from LinkedIn. You can download it in two ways:\n\n\n\nVisit the dataset page here:\nüëâ Kaggle - Data Science Job Postings and Skills\n\n\n\n\nIf you‚Äôre using Python, follow my step-by-step guide to downloading Kaggle datasets using the new kagglehub library:\nüëâ How to Download Datasets from Kaggle Using kagglehub\n\nOnce you‚Äôve downloaded the dataset, make sure the following CSV files are in your working directory:\n\njob_postings.csv\njob_skills.csv\njob_summary.csv"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#dataset-overview",
    "href": "posts/job_postings_analysis_part1.html#dataset-overview",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "This notebook is part of a multi-post series analyzing a dataset of real Data Science job postings collected from LinkedIn in 2024.\nIn this first part, we focus on exploring the structure of the job_posting.csv file and performing basic cleaning and exploratory analysis.\nColumns overview:\n\njob_link: direct link to the job posting\njob_title: job title\ncompany: company name\njob_location: job location (city/state/country)\nfirst_seen: when the job was first scraped\nAdditional metadata: processing status flags\n\nOur goal today is to clean the data and explore job titles, companies, and job locations.\nLoad the necessary libraries and (optional) set the preferred configurations for your data visualizations.\n\n# load the necessary libraries and configure the settings for data visualizations\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# viz. configurations\nsns.set(style=\"whitegrid\") # improves plots readability\nplt.rcParams[\"figure.figsize\"] = (10, 5) # this configuration is best suitable for blog posts like this\n\nLoad your data:\n\ndf = pd.read_csv(\"job_postings.csv\")\ndf.head(3)\n\n\n\n\n\n\n\n\njob_link\nlast_processed_time\nlast_status\ngot_summary\ngot_ner\nis_being_worked\njob_title\ncompany\njob_location\nfirst_seen\nsearch_city\nsearch_country\nsearch_position\njob_level\njob_type\n\n\n\n\n0\nhttps://www.linkedin.com/jobs/view/senior-mach...\n2024-01-21 08:08:48.031964+00\nFinished NER\nt\nt\nf\nSenior Machine Learning Engineer\nJobs for Humanity\nNew Haven, CT\n2024-01-14\nEast Haven\nUnited States\nAgricultural-Research Engineer\nMid senior\nOnsite\n\n\n1\nhttps://www.linkedin.com/jobs/view/principal-s...\n2024-01-20 04:02:12.331406+00\nFinished NER\nt\nt\nf\nPrincipal Software Engineer, ML Accelerators\nAurora\nSan Francisco, CA\n2024-01-14\nEl Cerrito\nUnited States\nSet-Key Driver\nMid senior\nOnsite\n\n\n2\nhttps://www.linkedin.com/jobs/view/senior-etl-...\n2024-01-21 08:08:31.941595+00\nFinished NER\nt\nt\nf\nSenior ETL Data Warehouse Specialist\nAdame Services LLC\nNew York, NY\n2024-01-14\nMiddletown\nUnited States\nTechnical Support Specialist\nAssociate\nOnsite\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 12217 entries, 0 to 12216\nData columns (total 15 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   job_link             12217 non-null  object\n 1   last_processed_time  12217 non-null  object\n 2   last_status          12217 non-null  object\n 3   got_summary          12217 non-null  object\n 4   got_ner              12217 non-null  object\n 5   is_being_worked      12217 non-null  object\n 6   job_title            12217 non-null  object\n 7   company              12217 non-null  object\n 8   job_location         12216 non-null  object\n 9   first_seen           12217 non-null  object\n 10  search_city          12217 non-null  object\n 11  search_country       12217 non-null  object\n 12  search_position      12217 non-null  object\n 13  job_level            12217 non-null  object\n 14  job_type             12217 non-null  object\ndtypes: object(15)\nmemory usage: 1.4+ MB"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#step-1-drop-irrelevant-columns",
    "href": "posts/job_postings_analysis_part1.html#step-1-drop-irrelevant-columns",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "As we will not use all the columns, we will now drop the unnecessary ones, reducing thus the volume of the dataset:\n\n# drop unnecessary columns\ndf_cleaned = df.drop(columns=[\n    'job_link', 'last_processed_time', 'last_status', \n    'got_summary', 'got_ner', 'is_being_worked'\n])"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#step-2-clean-duplicates-and-missing-values",
    "href": "posts/job_postings_analysis_part1.html#step-2-clean-duplicates-and-missing-values",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "Let‚Äôs now clean our dataset from duplicate values and look for missing values.\n\n# drop duplicates\ndf_cleaned = df_cleaned.drop_duplicates()\n\n\n# count missing values\ndf_cleaned.isnull().sum()\n\njob_title          0\ncompany            0\njob_location       0\nfirst_seen         0\nsearch_city        0\nsearch_country     0\nsearch_position    0\njob_level          0\njob_type           0\ndtype: int64\n\n\nWe found one missing value in job_location. This time we will just remove the missing row with teh missing value. In other contexts, it makes sense to deal with misisng values in other ways. You can read about methods to deal with missing values here: https://www.geeksforgeeks.org/ml-handling-missing-values/.\n\n# drop missing values\ndf_cleaned = df_cleaned.dropna()"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#step-3-explore-the-data",
    "href": "posts/job_postings_analysis_part1.html#step-3-explore-the-data",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "We will now have a deeper look at teh dataset, in particular focusing on:\n\njob titles\ncompanies\nlocation\n\nLet‚Äôs count the number of entries for each job title, company and location:\n\n# count entriep per job title, diplay the first 10\ndf_cleaned['job_title'].value_counts().head(10)\n\njob_title\nSenior Data Engineer                         273\nSenior Data Analyst                          157\nData Engineer                                146\nSenior MLOps Engineer                        138\nData Analyst                                 131\nData Scientist                               127\nSenior Data Scientist                        117\nLead Data Engineer                           116\nData Architect                               106\nStaff Machine Learning Engineer, Series A     99\nName: count, dtype: int64\n\n\n\n# count the entries for each company\ndf_cleaned['company'].value_counts().head(10)\n\ncompany\nJobs for Humanity            669\nRecruiting from Scratch      387\nDice                         179\nAgoda                        172\nClearanceJobs                159\nClickJobs.io                 152\nCapital One                   80\nEnergy Jobline                70\nDeloitte                      67\nAmazon Web Services (AWS)     64\nName: count, dtype: int64\n\n\n\n# count teh entries for each location\ndf_cleaned['job_location'].value_counts().head(10)\n\njob_location\nNew York, NY                       273\nChicago, IL                        232\nLondon, England, United Kingdom    230\nSan Francisco, CA                  196\nWashington, DC                     171\nAustin, TX                         165\nSeattle, WA                        160\nDallas, TX                         154\nBoston, MA                         150\nAtlanta, GA                        144\nName: count, dtype: int64\n\n\nWe can now visualize the results:\n\n# viz. job titles\ntop_titles = df_cleaned['job_title'].value_counts().head(10)\nsns.barplot(x=top_titles.values, y=top_titles.index, hue=top_titles.index, palette=\"viridis\", legend=False)\nplt.title(\"Top 10 Job Titles\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Job Title\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# viz. top 10 hiring companies\ntop_companies = df_cleaned['company'].value_counts().head(10)\nsns.barplot(x=top_companies.values, y=top_companies.index, hue=top_companies.index, palette=\"magma\", legend=False)\nplt.title(\"Top 10 Hiring Companies\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Company\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# top 10 job locations\ntop_locations = df_cleaned['job_location'].value_counts().head(10)\nsns.barplot(x=top_locations.values, y=top_locations.index, palette=\"cubehelix\", hue=top_locations.index, legend=False)\nplt.title(\"Top 10 Job Locations\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Location\")\nplt.show()"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#main-takeouts",
    "href": "posts/job_postings_analysis_part1.html#main-takeouts",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "We have seen that:\n\nThe Top 3 Job Titles in data Science are: Senior Data Engineer, Senrios Data Analyst, and Data Engineer.\nThe top 3 Companies are: Jobs For Humanity, Recruiting from Scratch, and Dice.\nThe top 3 Locations are: New York, Chicago, and London.\n\nThis analysis has its clear limitation, and may be highly biased by the language of the job search, i.e., English. So, be carefull when drawing conclusions from these data!"
  },
  {
    "objectID": "posts/job_postings_analysis_part1.html#next-steps",
    "href": "posts/job_postings_analysis_part1.html#next-steps",
    "title": "Data Science Job Postings 2024 (Part I)",
    "section": "",
    "text": "In the next post, we‚Äôll explore the job_skills.csv file to identify:\n\nThe most frequently required skills\nSkill trends by job title or level\nKeywords and technologies in demand for Data Scientists\n\nüìå Stay tuned!"
  }
]